{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from utilidades.calibration import utilities as ult\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import optuna \n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import sklearn.pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(DONE) * Lightgbm       - Algorítimico \n",
    "(DONE) * Smote          - Preprocessing\n",
    "\n",
    "- Vini\n",
    "* CDBH           - Híbrido - Email enviado - POSSIVEL IMPLEMENTAR https://sci-hub.se/10.1016/j.eswa.2020.114035\n",
    "* OSM Classifier - Algorítimico - Email enviado https://sci-hub.se/10.1016/j.eswa.2018.01.008 \n",
    "* SE NÃO TIVER COMO IMPLEMENTAR EU IMPLEMENTO S-CSL https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_6_ImbalancedLearning/CostSensitive.html\n",
    "\n",
    "(DONE)* SVM VR         - Preprocessing 100%.\n",
    "\n",
    "(STARTING)* DBSMOTE IN R      - Preprocessing Construir no R. 0% Terminar até Sexta.\n",
    "\n",
    "\n",
    "- Manaus\n",
    "(DOING)* HDDT Emsemble  - Algorítimico Implementado no R 0%\n",
    "(DOING)* CCR            - Preprocessing 15%\n",
    "\n",
    "(TO DO) POTENCIAL ANCHORING      - Preprocessing\n",
    "\n",
    "(DONE) * EasyEmsemble   - Algorítimico\n",
    "(DONE)* Método novo    - Algorítimico\n",
    "\n",
    "\n",
    "#### CANCELED's\n",
    "\n",
    "(CANCELED)* EHSO           - Preprocessing\n",
    "(CANCELED)* WHMBoost       - Cost Sensitivity EMAIL ENVIADO\n",
    "(CANCELED)* CHMDT          - Cost Sensitivity EMAIL ENVIADO\n",
    "\n",
    "https://www.rdocumentation.org/packages/smotefamily/versions/1.3.1/topics/DBSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-> Vini\n",
    "* CDBH                     - Híbrido               NA\n",
    "* OSM Classifier    - Algorítimico       NA\n",
    "* SVM VR                - Preprocessing   FEITO\n",
    "* DBSMOTE            - Preprocessing   INICIANDO\n",
    "\n",
    "-> Manaus\n",
    "* HDDT                    - Algorítimico        NA\n",
    "* CCR                       - Preprocessing    FEITO\n",
    "* POTENCIAL A.   - Preprocessing    FAZENDO\n",
    "* EasyEmsemble - Algorítimico         FEITO\n",
    "* Método novo     - Algorítimico         FEITO\n",
    "* Lightgbm            - Algorítimico         FEITO\n",
    "* Smote                  - Preprocessing     FEITO\n",
    "\n",
    "-> On hold\n",
    "* EHSO                   - Preprocessing\n",
    "* WHMBoost        - Cost Sensitivity \n",
    "* CHMDT               - Cost Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32) diagnosis\n",
      "B            357\n",
      "M            212\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('breast-cancer.csv') ; print(df.shape,df.select_dtypes('object').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.drop('id',axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "df.diagnosis.replace('B',0,inplace=True) # Benignos para 0\n",
    "df.diagnosis.replace('M',1,inplace=True) # Malignos para 1\n",
    "\n",
    "df = pd.read_csv('breast-cancer.csv') ; print(df.shape,df.select_dtypes('object').value_counts())\n",
    "df['target'] = np.where(df['diagnosis'] == 'B',0,1)\n",
    "df.drop(['diagnosis'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm_vr():\n",
    "    def __init__(self, dataframe : pd.DataFrame, target: str,metric : str = 'average_precision', pipe_final : sklearn.pipeline = None):\n",
    "        self.dataframe = dataframe\n",
    "        self.target = target\n",
    "        self.metric = metric\n",
    "        self._pipe_final = pipe_final\n",
    "\n",
    "    def fit(self,random_state=42):\n",
    "        # return y_train2 é o retorno dessa função. e utilizo como resposta\n",
    "        # Splito os dados aqui\n",
    "\n",
    "        X,y = ult.splitxy(self.dataframe,self.target)\n",
    "\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test, self.X_val, self.y_val = ult.train_test_val(X,y)\n",
    "\n",
    "        prep_feat_tuple = ult.create_prep_pipe2(self.dataframe,self.target)\n",
    "        self.prep_feat = prep_feat_tuple[0]\n",
    "\n",
    "        self.lists_pandarizer = list(prep_feat_tuple[1]) + list(prep_feat_tuple[2])\n",
    "\n",
    "        self.pipe_prep = Pipeline([\n",
    "                    ('transformer_prep', self.prep_feat),\n",
    "                    (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = self.lists_pandarizer))),\n",
    "                ])\n",
    "\n",
    "        # self.pipe_prep.fit(self.X_train) # Precisa do self?\n",
    "        \n",
    "        self.y_train2 = self.svm_vr_y()\n",
    "\n",
    "        LGBM = LGBMClassifier(random_state = 42, n_jobs = -1)\n",
    "\n",
    "        pipe_tuning = Pipeline([\n",
    "            ('transformer_prep', self.prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = self.lists_pandarizer))),\n",
    "            ('estimator', LGBM)\n",
    "        ])\n",
    "        \n",
    "        self.pipe_prep = Pipeline([\n",
    "            ('transformer_prep', self.prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = self.lists_pandarizer))),\n",
    "        ])\n",
    "        self.pipe_prep.fit(self.X_train)       \n",
    "        \n",
    "        cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.3, random_state = 42)\n",
    "        \n",
    "        metric = self.metric\n",
    "        \n",
    "        fit_params = {\n",
    "            'eval_metric': metric, \n",
    "            'eval_set': [(self.X_test, pd.DataFrame(self.y_test))],\n",
    "            'callbacks': [(early_stopping(stopping_rounds = 10, verbose = True))],\n",
    "        }        \n",
    "        \n",
    "        LGBM_search_space = {\n",
    "            \"estimator__learning_rate\": Real(0.001, 0.01, prior = 'log-uniform'),\n",
    "            \"estimator__n_estimators\": Integer(100, 1000),\n",
    "            \"estimator__class_weight\": Categorical(['balanced', None]),\n",
    "            \"estimator__num_leaves\": Integer(32, 256),\n",
    "            \"estimator__min_child_samples\": Integer(100, 1000),\n",
    "            \"estimator__reg_alpha\": Real(0, 100, prior = 'uniform'),\n",
    "            \"estimator__reg_lambda\": Real(10., 200., prior = 'uniform'),\n",
    "            \"estimator__objective\": Categorical(['binary']),\n",
    "            \"estimator__importance_type\":Categorical(['gain']),\n",
    "            \"estimator__boosting_type\": Categorical(['goss'])\n",
    "        }    \n",
    "        \n",
    "        LGBM_bayes_search = BayesSearchCV(pipe_tuning, LGBM_search_space, n_iter = 2, scoring = metric, \n",
    "                                         return_train_score = True, \n",
    "                                         fit_params = fit_params,\n",
    "                                         n_jobs = -1, cv = cv, random_state = 42, optimizer_kwargs = {'base_estimator': 'GP'})\n",
    "\n",
    "        # Testing if it's compatible\n",
    "\n",
    "        from collections import Counter\n",
    "        l = Counter(self.y_train2)\n",
    "\n",
    "        if len(l.keys()) == 1:\n",
    "            print('The model was not Abble to fit correctly because the SVM got all targets with the same value')\n",
    "            \n",
    "            LGBM_bayes_search.fit(self.X_train, self.y_train)        \n",
    "            \n",
    "            results_cv = pd.DataFrame(LGBM_bayes_search.cv_results_)\n",
    "            \n",
    "            temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "            temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "            to_go = temp[abs(temp['diff']) < 0.05].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "            \n",
    "            params = results_cv.loc[to_go.values[0]]\n",
    "            kwargs = params.params   \n",
    "            \n",
    "            best_LGBM = LGBMClassifier(random_state = 42, n_jobs = -1, verbose = -1, **kwargs)\n",
    "            \n",
    "            best_LGBM.fit(self.pipe_prep.transform(self.X_train), self.y_train, early_stopping_rounds = 10, verbose = 20, eval_metric = metric,\n",
    "                        eval_set = [(self.pipe_prep.transform(self.X_test), self.y_test)]) \n",
    "            \n",
    "            \n",
    "            pipe_final = Pipeline(\n",
    "            [\n",
    "                ('pipe_transformer_prep', self.pipe_prep),\n",
    "                ('pipe_estimator', best_LGBM)\n",
    "            ])       \n",
    "            \n",
    "            self._pipe_final = pipe_final\n",
    "\n",
    "        else:\n",
    "            LGBM_bayes_search.fit(self.X_train, self.y_train2)        \n",
    "            \n",
    "            results_cv = pd.DataFrame(LGBM_bayes_search.cv_results_)\n",
    "            \n",
    "            temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "            temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "            to_go = temp[abs(temp['diff']) < 0.05].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "            \n",
    "            params = results_cv.loc[to_go.values[0]]\n",
    "            kwargs = params.params   \n",
    "            \n",
    "            best_LGBM = LGBMClassifier(random_state = 42, n_jobs = -1, verbose = -1, **kwargs)\n",
    "            \n",
    "            best_LGBM.fit(self.pipe_prep.transform(self.X_train), self.y_train, early_stopping_rounds = 10, verbose = 20, eval_metric = metric,\n",
    "                        eval_set = [(self.pipe_prep.transform(self.X_test), self.y_test)]) \n",
    "            \n",
    "            \n",
    "            pipe_final = Pipeline(\n",
    "            [\n",
    "                ('pipe_transformer_prep', self.pipe_prep),\n",
    "                ('pipe_estimator', best_LGBM)\n",
    "            ])       \n",
    "            \n",
    "            self._pipe_final = pipe_final\n",
    "\n",
    "    def svm_vr_y(self,random_state=42):\n",
    "        \"\"\" This function fits a SVM Classifier, taking the best result from the grid searching.\n",
    "        After the best fit has been achieved, the training dependent variable values must be replaced by the predicts of the trained model, thus obtaining the balancing.\n",
    "\n",
    "        Following the instructions of the Authors.\n",
    "        They suggest using higher values for C and the RBF kernel function.\n",
    "\n",
    "        \"The proposed methodology followed a two phase\n",
    "        approach. During the first phase the available training data is used\n",
    "        to train SVM. Later, the target values of the training data are replaced\n",
    "        by the corresponding predictions of the trained SVM. During the\n",
    "        second phase, this modified data is used to train MLP, LR, and RF\n",
    "        separately\" Authors.\n",
    "\n",
    "        Based on the ideia by the authors, we select the best model, by the value of accuracy\n",
    "        \"\"\"\n",
    "        metric = self.metric\n",
    "        SVM = SVC(random_state=42)\n",
    "\n",
    "        pipe_tuning = Pipeline([\n",
    "            ('transformer_prep', self.prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = self.lists_pandarizer))),\n",
    "            ('estimator', SVM)\n",
    "        ])\n",
    "\n",
    "        SVC_search_space = {\n",
    "            \"estimator__C\": Integer(5, 10),   \n",
    "            \"estimator__gamma\": Integer(5, 10),\n",
    "            \"estimator__degree\": Integer(1,8),\n",
    "            \"estimator__kernel\": Categorical(['linear', 'poly', 'rbf','sigmoid'])\n",
    "        }   \n",
    "\n",
    "        SVM_vr_search = BayesSearchCV(\n",
    "            pipe_tuning,\n",
    "            SVC_search_space,   \n",
    "            n_iter=2,\n",
    "            random_state=42,\n",
    "            cv=5,\n",
    "            scoring=metric,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        SVM_vr_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        results_cv = pd.DataFrame(SVM_vr_search.cv_results_)\n",
    "        \n",
    "        temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "        temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "        to_go = temp[abs(temp['diff']) < 0.2].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "        \n",
    "        params = results_cv.loc[to_go.values[0]]\n",
    "        kwargs = params.params\n",
    "        kwargs = collections.OrderedDict((key.replace('estimator__', ''), value) for key, value in kwargs.items())\n",
    "\n",
    "        best_SVM = SVC(random_state=random_state,**kwargs).fit(self.X_train,self.y_train)\n",
    "        y_train2= best_SVM.predict(self.X_train)\n",
    "        return y_train2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_testing = svm_vr(df, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "The model was not Abble to fit correctly because the SVM got all targets with the same value\n",
      "[20]\tvalid_0's average_precision: 0.971402\tvalid_0's binary_logloss: 0.223291\n"
     ]
    }
   ],
   "source": [
    "svm_testing.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "\n",
    "#### Treino Teste a Val. CHECK\n",
    "\n",
    "- Preprocesso Treino \n",
    "- Teste para hiperparâmetro.\n",
    "- Validação checo a performance do modelo\n",
    "\n",
    "\n",
    "#### Adicionar o fit_params no BayesSearchCV\n",
    "#### Adicionar o RF classifier (Predict e validação final) sem Tunning CHECK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

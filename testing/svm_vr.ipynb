{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from utilidades.calibration import utilities as ult\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import optuna \n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import sklearn.pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import early_stopping\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(DONE) * Lightgbm       - Algorítimico \n",
    "(DONE) * Smote          - Preprocessing\n",
    "\n",
    "- Vini\n",
    "* CDBH           - Híbrido - Mandar email\n",
    "* OSM Classifier - Algorítimico - Mandar email\n",
    "\n",
    "(DONE)* SVM VR         - Preprocessing 98%. Testando a Classe\n",
    "\n",
    "(STARTING)* DBSMOTE IN R      - Preprocessing Construir no R. 0%\n",
    "\n",
    "\n",
    "- Manaus\n",
    "(DOING)* HDDT Emsemble  - Algorítimico Implementado no R 0%\n",
    "(DOING)* CCR            - Preprocessing 15%\n",
    "\n",
    "(TO DO) POTENCIAL ANCHORING      - Preprocessing\n",
    "\n",
    "(DONE) * EasyEmsemble   - Algorítimico\n",
    "(DONE)* Método novo    - Algorítimico\n",
    "\n",
    "\n",
    "#### CANCELED's\n",
    "\n",
    "(CANCELED)* EHSO           - Preprocessing\n",
    "(CANCELED)* WHMBoost       - Cost Sensitivity EMAIL ENVIADO\n",
    "(CANCELED)* CHMDT          - Cost Sensitivity EMAIL ENVIADO\n",
    "\n",
    "https://www.rdocumentation.org/packages/smotefamily/versions/1.3.1/topics/DBSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ = df.drop('diagnosis', axis=1)\n",
    "# y = df[['diagnosis']]\n",
    "\n",
    "# X = np.asarray(X_)\n",
    "# y = np.asarray(y).ravel()\n",
    "\n",
    "# X_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=32,stratify=y)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# SVM_vr_search = BayesSearchCV(\n",
    "#     SVC(),\n",
    "#     {\n",
    "#         'C': Integer(5,10),\n",
    "#         'gamma': Integer(5,10),\n",
    "#         'degree': Integer(1,8),\n",
    "#         'kernel': Categorical(['linear', 'poly', 'rbf','sigmoid'])\n",
    "#     },\n",
    "#      n_iter=32,\n",
    "#      random_state=32,\n",
    "#      cv=5,\n",
    "#      scoring='accuracy',\n",
    "#      return_train_score=True\n",
    "# )\n",
    "\n",
    "# SVM_vr_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# X_test_scaled = scaler.transform(x_test)\n",
    "# print(SVM_vr_search.score(X_test_scaled, y_test))\n",
    "\n",
    "# results_cv = pd.DataFrame(SVM_vr_search.cv_results_)\n",
    "\n",
    "# temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "# temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "# to_go = temp[abs(temp['diff']) < 0.05].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "\n",
    "# params = results_cv.loc[to_go.values[0]]\n",
    "# kwargs = params.params\n",
    "# print(kwargs)\n",
    "\n",
    "# best_SVM = SVC(random_state=32,**kwargs).fit(X_train_scaled,y_train)\n",
    "# y_train2= best_SVM.predict(X_train_scaled)\n",
    "# X_testz = scaler.inverse_transform(X_train_scaled)\n",
    "# new_train = pd.DataFrame(X_testz,columns=X_.columns)\n",
    "# new_train['y'] = y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32) diagnosis\n",
      "B            357\n",
      "M            212\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('breast-cancer.csv') ; print(df.shape,df.select_dtypes('object').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[float, int]).columns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "if num_cols.values.shape[0] > 0:\n",
    "            pipe_num = Pipeline(\n",
    "            steps = [\n",
    "                (\"selector_num\", ColumnTransformer([(\"selector\", \"passthrough\", num_cols.values)], remainder = 'drop')),\n",
    "                ('num_imputer', SimpleImputer(strategy='mean'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.drop('id',axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "df.diagnosis.replace('B',0,inplace=True) # Benignos para 0\n",
    "df.diagnosis.replace('M',1,inplace=True) # Malignos para 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\vinic\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "c:\\Users\\vinic\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9627659574468085\n",
      "OrderedDict([('C', 5), ('degree', 1), ('gamma', 10), ('kernel', 'linear')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinic\\AppData\\Local\\Temp/ipykernel_12368/1773425698.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n"
     ]
    }
   ],
   "source": [
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_ = df.drop('diagnosis', axis=1)\n",
    "y = df[['diagnosis']]\n",
    "\n",
    "X = np.asarray(X_)\n",
    "y = np.asarray(y).ravel()\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=32,stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "SVM_vr_search = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'C': Integer(5,10),\n",
    "        'gamma': Integer(5,10),\n",
    "        'degree': Integer(1,8),\n",
    "        'kernel': Categorical(['linear', 'poly', 'rbf','sigmoid'])\n",
    "    },\n",
    "     n_iter=32,\n",
    "     random_state=32,\n",
    "     cv=5,\n",
    "     scoring='accuracy',\n",
    "     return_train_score=True\n",
    ")\n",
    "\n",
    "SVM_vr_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "print(SVM_vr_search.score(X_test_scaled, y_test))\n",
    "\n",
    "results_cv = pd.DataFrame(SVM_vr_search.cv_results_)\n",
    "\n",
    "temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "to_go = temp[abs(temp['diff']) < 0.05].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "\n",
    "params = results_cv.loc[to_go.values[0]]\n",
    "kwargs = params.params\n",
    "print(kwargs)\n",
    "\n",
    "best_SVM = SVC(random_state=32,**kwargs).fit(X_train_scaled,y_train)\n",
    "y_train2= best_SVM.predict(X_train_scaled)\n",
    "X_testz = scaler.inverse_transform(X_train_scaled)\n",
    "new_train = pd.DataFrame(X_testz,columns=X_.columns)\n",
    "new_train['y'] = y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = ult.splitxy(df,'diagnosis')\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = ult.train_test_val(X,y)\n",
    "\n",
    "prep_feat_tuple = ult.create_prep_pipe(df,'diagnosis')\n",
    "prep_feat = prep_feat_tuple[0]\n",
    "\n",
    "lists_pandarizer = list(prep_feat_tuple[1]) + list(prep_feat_tuple[2])\n",
    "\n",
    "pipe_prep = Pipeline([\n",
    "            ('transformer_prep', prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = lists_pandarizer))),\n",
    "        ])\n",
    "pipe_prep.fit(X_train)\n",
    "\n",
    "#X_train = np.asarray(X_train)\n",
    "\n",
    "SVC(random_state=42)\n",
    "\n",
    "metric = 'accuracy'\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': metric, \n",
    "    'eval_set': [(X_test, pd.DataFrame(y_test))],\n",
    "}\n",
    "\n",
    "pipe_tuning = Pipeline([\n",
    "    ('transformer_prep', prep_feat),\n",
    "    (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = lists_pandarizer))),\n",
    "    ('estimator', SVC())\n",
    "])\n",
    "\n",
    "SVC_search_space = {\n",
    "    \"estimator__C\": Integer(5, 10),   \n",
    "    \"estimator__gamma\": Integer(5, 10),\n",
    "    \"estimator__degree\": Integer(1,8),\n",
    "    \"estimator__kernel\": Categorical(['linear', 'poly', 'rbf','sigmoid'])\n",
    "}   \n",
    "\n",
    "SVM_vr_search = BayesSearchCV(\n",
    "    pipe_tuning,\n",
    "    SVC_search_space,   \n",
    "    n_iter=2,\n",
    "    random_state=42,\n",
    "    cv=5,\n",
    "    scoring=metric,\n",
    "    return_train_score=True,\n",
    "    fit_params = fit_params,\n",
    ")\n",
    "SVM_vr_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm_vr():\n",
    "    def __init__(self, dataframe : pd.DataFrame, target: str,metric : str = 'accuracy', pipe_final : sklearn.pipeline = None):\n",
    "        self.dataframe = dataframe\n",
    "        self.target = target\n",
    "        self.metric = metric\n",
    "        self._pipe_final = pipe_final\n",
    "\n",
    "    def fit(self,random_state=42):\n",
    "        # return y_train2 é o retorno dessa função. e utilizo como resposta\n",
    "        # Splito os dados aqui\n",
    "\n",
    "        X,y = ult.splitxy(self.dataframe,self.target)\n",
    "\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test, self.X_val, self.y_val = ult.train_test_val(X,y)\n",
    "\n",
    "        prep_feat_tuple = ult.create_prep_pipe2(self.dataframe,self.target)\n",
    "        self.prep_feat = prep_feat_tuple[0]\n",
    "\n",
    "        lists_pandarizer = list(prep_feat_tuple[1]) + list(prep_feat_tuple[2])\n",
    "\n",
    "        self.pipe_prep = Pipeline([\n",
    "                    ('transformer_prep', self.prep_feat),\n",
    "                    (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = lists_pandarizer))),\n",
    "                ])\n",
    "\n",
    "        print(self.X_train)\n",
    "        # self.pipe_prep.fit(self.X_train) # Precisa do self?\n",
    "\n",
    "        y_train2 = svm_vr()\n",
    "\n",
    "        LGBM = LGBMClassifier(random_state = 42, n_jobs = -1)\n",
    "\n",
    "        pipe_tuning = Pipeline([\n",
    "            ('transformer_prep', self.prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = lists_pandarizer))),\n",
    "            ('estimator', LGBM)\n",
    "        ])\n",
    "        \n",
    "        self.pipe_prep = Pipeline([\n",
    "            ('transformer_prep', self.prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = lists_pandarizer))),\n",
    "        ])\n",
    "        self.pipe_prep.fit(X_train)       \n",
    "        \n",
    "        cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.3, random_state = 42)\n",
    "        \n",
    "        metric = self.metric\n",
    "        \n",
    "        fit_params = {\n",
    "            'eval_metric': metric, \n",
    "            'eval_set': [(self.X_test, pd.DataFrame(self.y_test))],\n",
    "            'callbacks': [(early_stopping(stopping_rounds = 10, verbose = True))],\n",
    "        }        \n",
    "        \n",
    "        LGBM_search_space = {\n",
    "            \"estimator__learning_rate\": Real(0.001, 0.01, prior = 'log-uniform'),\n",
    "            \"estimator__n_estimators\": Integer(100, 1000),\n",
    "            \"estimator__class_weight\": Categorical(['balanced', None]),\n",
    "            \"estimator__num_leaves\": Integer(32, 256),\n",
    "            \"estimator__min_child_samples\": Integer(100, 1000),\n",
    "            \"estimator__reg_alpha\": Real(0, 100, prior = 'uniform'),\n",
    "            \"estimator__reg_lambda\": Real(10., 200., prior = 'uniform'),\n",
    "            \"estimator__objective\": Categorical(['binary']),\n",
    "            \"estimator__importance_type\":Categorical(['gain']),\n",
    "            \"estimator__boosting_type\": Categorical(['goss'])\n",
    "        }    \n",
    "        \n",
    "        LGBM_bayes_search = BayesSearchCV(pipe_tuning, LGBM_search_space, n_iter = 2, scoring = metric, \n",
    "                                         return_train_score = True, \n",
    "                                         fit_params = fit_params,\n",
    "                                         n_jobs = -1, cv = cv, random_state = 42, optimizer_kwargs = {'base_estimator': 'GP'})\n",
    "        \n",
    "        \n",
    "        LGBM_bayes_search.fit(self.X_train, self.y_train2)        \n",
    "        \n",
    "        results_cv = pd.DataFrame(LGBM_bayes_search.cv_results_)\n",
    "        \n",
    "        temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "        temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "        to_go = temp[abs(temp['diff']) < 0.05].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "        \n",
    "        params = results_cv.loc[to_go.values[0]]\n",
    "        kwargs = params.params   \n",
    "        print(kwargs)\n",
    "        \n",
    "        best_LGBM = LGBMClassifier(random_state = 42, n_jobs = -1, verbose = -1, **kwargs)\n",
    "        \n",
    "        best_LGBM.fit(self.pipe_prep.transform(self.X_train), self.y_train, early_stopping_rounds = 10, verbose = 20, eval_metric = metric,\n",
    "                     eval_set = [(self.pipe_prep.transform(self.X_test), self.y_test)]) \n",
    "        \n",
    "        \n",
    "        pipe_final = Pipeline(\n",
    "        [\n",
    "            ('pipe_transformer_prep', self.pipe_prep),\n",
    "            ('pipe_estimator', best_LGBM)\n",
    "        ])       \n",
    "        \n",
    "        self._pipe_final = pipe_final\n",
    "\n",
    "    def svm_vr(self,random_state=42):\n",
    "        \"\"\" This function fits a SVM Classifier, taking the best result from the grid searching.\n",
    "        After the best fit has been achieved, the training dependent variable values must be replaced by the predicts of the trained model, thus obtaining the balancing.\n",
    "\n",
    "        Following the instructions of the Authors.\n",
    "        They suggest using higher values for C and the RBF kernel function.\n",
    "\n",
    "        \"The proposed methodology followed a two phase\n",
    "        approach. During the first phase the available training data is used\n",
    "        to train SVM. Later, the target values of the training data are replaced\n",
    "        by the corresponding predictions of the trained SVM. During the\n",
    "        second phase, this modified data is used to train MLP, LR, and RF\n",
    "        separately\" Authors.\n",
    "\n",
    "        Based on the ideia by the authors, we select the best model, by the value of accuracy\n",
    "        \"\"\"\n",
    "        metric = self.metric\n",
    "\n",
    "        SVM = SVC(random_state=42)\n",
    "\n",
    "        pipe_tuning = Pipeline([\n",
    "            ('transformer_prep', self.prep_feat),\n",
    "            (\"pandarizer\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = lists_pandarizer))),\n",
    "            ('estimator', SVM)\n",
    "        ])\n",
    "\n",
    "        SVC_search_space = {\n",
    "            \"estimator__C\": Integer(5, 10),   \n",
    "            \"estimator__gamma\": Integer(5, 10),\n",
    "            \"estimator__degree\": Integer(1,8),\n",
    "            \"estimator__kernel\": Categorical(['linear', 'poly', 'rbf','sigmoid'])\n",
    "        }   \n",
    "\n",
    "        SVM_vr_search = BayesSearchCV(\n",
    "            pipe_tuning,\n",
    "            SVC_search_space,   \n",
    "            n_iter=2,\n",
    "            random_state=42,\n",
    "            cv=5,\n",
    "            scoring=metric,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "        SVM_vr_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        results_cv = pd.DataFrame(SVM_vr_search.cv_results_)\n",
    "        \n",
    "        temp = results_cv[['mean_train_score', 'mean_test_score']]\n",
    "        temp['diff'] = temp['mean_test_score'] - temp['mean_train_score']\n",
    "        to_go = temp[abs(temp['diff']) < 0.05].sort_values(by = 'mean_test_score', ascending = False).head(1).index\n",
    "        \n",
    "        params = results_cv.loc[to_go.values[0]]\n",
    "        kwargs = params.params\n",
    "        print(kwargs)\n",
    "\n",
    "        best_SVM = SVC(random_state=random_state,**kwargs).fit(X_train,y_train)\n",
    "        y_train2= best_SVM.predict(X_train)\n",
    "        return y_train2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_testing = svm_vr(df, 'diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "473        12.27         29.97           77.42      465.4          0.07699   \n",
      "89         14.64         15.24           95.77      651.9          0.11320   \n",
      "566        16.60         28.08          108.30      858.1          0.08455   \n",
      "189        12.30         15.90           78.83      463.7          0.08080   \n",
      "493        12.46         12.83           78.83      477.3          0.07372   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "166        10.80          9.71           68.77      357.6          0.09594   \n",
      "332        11.22         19.86           71.94      387.3          0.10540   \n",
      "465        13.24         20.13           86.87      542.9          0.08284   \n",
      "335        17.06         21.00          111.80      918.6          0.11190   \n",
      "312        12.76         13.37           82.29      504.1          0.08794   \n",
      "\n",
      "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "473           0.03398        0.000000             0.000000         0.1701   \n",
      "89            0.13390        0.099660             0.070640         0.2116   \n",
      "566           0.10230        0.092510             0.053020         0.1590   \n",
      "189           0.07253        0.038440             0.016540         0.1667   \n",
      "493           0.04043        0.007173             0.011490         0.1613   \n",
      "..                ...             ...                  ...            ...   \n",
      "166           0.05736        0.025310             0.016980         0.1381   \n",
      "332           0.06779        0.005006             0.007583         0.1940   \n",
      "465           0.12230        0.101000             0.028330         0.1601   \n",
      "335           0.10560        0.150800             0.099340         0.1727   \n",
      "312           0.07948        0.040520             0.025480         0.1601   \n",
      "\n",
      "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
      "473                 0.05960  ...         13.45          38.05   \n",
      "89                  0.06346  ...         16.34          18.24   \n",
      "566                 0.05648  ...         18.98          34.12   \n",
      "189                 0.05474  ...         13.35          19.59   \n",
      "493                 0.06013  ...         13.19          16.36   \n",
      "..                      ...  ...           ...            ...   \n",
      "166                 0.06400  ...         11.60          12.02   \n",
      "332                 0.06028  ...         11.98          25.78   \n",
      "465                 0.06432  ...         15.44          25.50   \n",
      "335                 0.06071  ...         20.99          33.15   \n",
      "312                 0.06140  ...         14.19          16.40   \n",
      "\n",
      "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "473            85.08       558.9           0.09422            0.05213   \n",
      "89            109.40       803.6           0.12770            0.30890   \n",
      "566           126.70      1124.0           0.11390            0.30940   \n",
      "189            86.65       546.7           0.10960            0.16500   \n",
      "493            83.24       534.0           0.09439            0.06477   \n",
      "..               ...         ...               ...                ...   \n",
      "166            73.66       414.0           0.14360            0.12570   \n",
      "332            76.91       436.1           0.14240            0.09669   \n",
      "465           115.00       733.5           0.12010            0.56460   \n",
      "335           143.20      1362.0           0.14490            0.20530   \n",
      "312            92.04       618.8           0.11940            0.22080   \n",
      "\n",
      "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "473          0.00000               0.00000          0.2409   \n",
      "89           0.26040               0.13970          0.3151   \n",
      "566          0.34030               0.14180          0.2218   \n",
      "189          0.14230               0.04815          0.2482   \n",
      "493          0.01674               0.02680          0.2280   \n",
      "..               ...                   ...             ...   \n",
      "166          0.10470               0.04603          0.2090   \n",
      "332          0.01335               0.02022          0.3292   \n",
      "465          0.65560               0.13570          0.2845   \n",
      "335          0.39200               0.18270          0.2623   \n",
      "312          0.17690               0.08411          0.2564   \n",
      "\n",
      "     fractal_dimension_worst  \n",
      "473                  0.06743  \n",
      "89                   0.08473  \n",
      "566                  0.07820  \n",
      "189                  0.06306  \n",
      "493                  0.07028  \n",
      "..                       ...  \n",
      "166                  0.07699  \n",
      "332                  0.06522  \n",
      "465                  0.12490  \n",
      "335                  0.07599  \n",
      "312                  0.08253  \n",
      "\n",
      "[255 rows x 30 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\Documents\\Projetos\\TCC\\utilidades\\calibration.py:86: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  cat_cols = dataframe.select_dtypes(include=[object, pd.datetime]).columns\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'dataframe' and 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13356/1674109040.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msvm_testing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13356/1230846516.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, random_state)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# self.pipe_prep.fit(self.X_train) # Precisa do self?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0my_train2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_vr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mLGBM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'dataframe' and 'target'"
     ]
    }
   ],
   "source": [
    "svm_testing.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalizar as variavéis caso apareça no X variáveis categoricas CHECK\n",
    "#### Entender o pipe do Manaus - Para esse tipo de variáveis. CHECK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "\n",
    "#### Treino Teste a Val. CHECK\n",
    "\n",
    "- Preprocesso Treino \n",
    "- Teste para hiperparâmetro.\n",
    "- Validação checo a performance do modelo\n",
    "\n",
    "\n",
    "#### Adicionar o fit_params no BayesSearchCV\n",
    "#### Adicionar o RF classifier (Predict e validação final) sem Tunning CHECK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
